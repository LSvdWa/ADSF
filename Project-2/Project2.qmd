---
title: "Predicting Diabetes"
authors:
- "Demi van den Biggelaar (9660089)"
- "August Gesthuizen (5292565)"
- "Friso Harff (7526946)"
- "Leander van der Waal (7180063)"
date: "2026-01-15"
format:
  html:
    embed-resources: true
---

# **Prediction of glucose using linear regression**

Demi van den Biggelaar (9660089)\
August Gesthuizen (5292565)\
Friso Harff (7526946)\
Leander van der Waal (7180063)\

## **Introduction**

1 in 14 Dutch citizens has diabetes (Cijfers Over Diabetes, z.d.). To treat diabetes more effectively and to prevent serious complications it is crucial to diagnose it as early as possible. Having diabetes means that there is a shortage of insulin in the body, causing glucose to stay in the blood instead, which can cause a range of complications (Umcu, z.d.).

We want to see if we can predict whether someone has diabetes using logistic regression.

To see if this is possible we use a dataset about diabetes and we use the different parameters to verify whether we can predict diabetes (Akturk & National Institute of Diabetes and Digestive and Kidney Diseases, 2020). The dataset is filtered on only females of at least 21 years of age from a Pima Indian heritage.

The different parameters in the dataset are:

-   Pregnancies: Number of times pregnant

-   Glucose: Plasma glucose concentration after 2 hours in an oral glucose tolerance test

-   BloodPressure: Diastolic blood pressure (mm Hg)

-   SkinThickness: Triceps skin fold thickness (mm)

-   Insulin: 2-Hour serum insulin (mu U/ml)

-   BMI: Body mass index (weight in kg/(height in m)\^2)

-   DiabetesPedigreeFunction: Diabetes pedigree function

-   Age: Age (years)

-   Outcome: Class variable (0 or 1, where 1 means positive for diabetes)

Our research question is as follows: can we predict whether someone has diabetes based on the parameters given in the dataset, namely: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and outcome or a subset of these paramaters using logistic regression?

## **Preprocessing & Loading data**

To preprocess the data we categorized the BMI into four categories, being underweight, healthy, overweight and obese. We also remove entries where we see that values are clearly skewed, for example a BMI of 0 should not exist.

------------------------------------------------------------------------

```{r}
# Initialization
setwd(".")
library(dplyr)
library(ggplot2)
library(magrittr)
library(kableExtra)
library(pROC)
library(regclass)
library(caret)
```

```{r}
#| warning: false

# Load .CSV and save as .RDS

datacsv <- read.csv("./data/raw/diabetes.csv", header = TRUE)
saveRDS(datacsv, "./data/raw/dataset.rds")

# Preprocessing
dataRDS <- readRDS("./data/raw/dataset.rds")
dataRDS <- na.omit(dataRDS) # removes NA from important columns
saveRDS(dataRDS, "./data/processed/dataset.rds")
dataset <- readRDS("./data/processed/dataset.rds")

# Recoding BMI to classes
dataRDS$BMI[dataRDS$BMI <= 18.5 ] <- 1                     # Underweight
dataRDS$BMI[dataRDS$BMI > 18.5 & dataRDS$BMI <= 25  ] <- 2 # Healthy
dataRDS$BMI[dataRDS$BMI > 25 & dataRDS$BMI <= 30 ] <- 3    # Overweight
dataRDS$BMI[dataRDS$BMI > 30] <- 4                         # Obese

# Removing missing values
dataRDS <- subset(dataRDS,
                     BloodPressure != 0 &
                     SkinThickness != 0 &
                     Glucose != 0 &
                     Insulin != 0 &
                     BMI != 0)

summary(dataset)
```

## **Exploring the data**

```{r}
#| warning: false

dataRDS
for (i in 1:ncol(dataRDS)) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
pairs(dataRDS)
```

Above you can see multiple ways that our variables are described. They are represented in two ways: using histograms and using.a panel. In the histograms you can see which values are representative of each variable while in the panel you can see the relation between of each variable with each other variable. The histograms will be briefly discussed, however remaining discussion will focus only on the relevant plots from the panel which we use for regression.

-   **Pregnancies:** number of times pregnant, represented as an integer, values range between 0-14

-   **Glucose:** plasma glucose concentration in a toleration test represented as an integer, values range from 56-198.

-   **BloodPressure:** Bloodpressure represented as an integer with values in the range of 24-110.

-   **Skinthickness:** Thickness of the skin in millimetres, represented as an integer with values ranging from 7-63.

-   **Insulin:** “2 hour serum insulin” with values ranging from 14-846.

-   **BMI:** BMI value represented as classification, namely: underweight=1, healthy=2, overweight=3 and obese=4. Most people in this study are considered to be obese.

-   **DiabetesPedigreeFunction:** Quantified genetic risk of diabetes. Risks range from 0-1,75. 

-   **Age:** Integer representing age, values range from 20-70.

-   **Outcome:** 0 or 1 value representing if someone has diabetes or not. Most participants don’t have diabetes.

-   **Moderation:** Product of the following values: BMI, Insulin, SkinThickness and BloodPressure. This value is added to support the regression accuracy.

## **Building the model**

```{r}
fit0 <- glm(Outcome ~ Age + Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction,  
            family = binomial,
            data = dataRDS)

fit1 <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction,  
            family = binomial,
            data = dataRDS)

fit2 <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction + Glucose*BMI,
            family = binomial,
            data = dataRDS)

fit3 <- glm(Outcome ~ Glucose + BMI + DiabetesPedigreeFunction, 
            family = binomial,
            data = dataRDS)

fit4 <- glm(Outcome ~ Glucose + BMI,
            family = binomial,
            data = dataRDS)
AIC(fit0)
BIC(fit0)

# fit1 is best
AIC(fit1)
BIC(fit1)
summary(fit1)

AIC(fit2)
BIC(fit2)
AIC(fit3)
BIC(fit3)
AIC(fit4)
BIC(fit4)
```

EXPLAIN WHY 1 IS BETTER THAN ALL OTHERS

## **Assumptions**

### Binary dependent variable

The first assumption in a logistic regression is that the outcome should be binary and therefore follow a binomial distribution. This is easy to check: you just need to be sure that the outcome can only take one of two responses. In our case, the possible outcomes are:

-   Diabetes (coded 1)

-   Not diabetes (coded 0)

```{r}
# Mutating Outcome into factor (necessary for next part)
dataRDS$Outcome <- as.factor(dataRDS$Outcome)

dataRDS %>% 
  ggplot(aes(x = Outcome, fill = Outcome)) +
  geom_bar() +
  labs(x = "Outcome",
       y = "Count",
       title = "Distribution of the Outcome variable") +
  theme_bw()
# You can see that there are indeed only two outcomes for `Outcome`, so our outcome follows a binomial distribution.
```

### Balanced outcomes

When using logistic regression to make predictions, then the accuracy will be affected by imbalance in the outcome classes. Notice that in the plot we just made there are more people who do not have diabetes than who do. A possible consequence is reduced accuracy in classification of diabetes.

A certain amount of imbalance is expected and can be handled well by the model in most cases. The effects of this imbalance is context-dependable. Some solutions to serious class imbalance are down-sampling or weighting the outcomes to balance the importance placed on the outcomes by the model. In this case, we consider the imbalance to be not extreme enough to not be handled sufficiently by the model.

### Sufficiently large sample size

Sample size in logistic regression is a complex issue, but some suggest that it is ideal to have 10 cases per candidate predictor in your model. The minimum number of cases to include is $N=\frac{10k}{p}$, where $k$ is the number of predictors and $p$ is the smallest proportion of negative or positive cases in the population.

```{r}
#First we need to get the proportion of people that have diabetes in our sample, which is 0.33

dataRDS %>% 
  count(Outcome) %>% 
  mutate(prop = n / sum(n))
```

```{r}
#Now we can plug this into our formula to get the minimum number of positive cases. 
ssize_cal <- function(k, p){
  round((10*k)/p)
}

ssize_cal(2, 0.33)
```

As can be seen in the Distribution of the Outcome variable chart, we have clearly more than the required 61 positive cases. To be exact we have 130 positive cases for diabetes. This means that our sample size is large enough.

### Predictor matrix is full-rank

For logistic regression it is important that there need to be more observations than predictors and there should be no multicollinearity among the linear predictors.

```{r}
VIF(fit1)
```

EXPLAIN WHY WHEN IT WORKS AS INTENDED, NEED ANSWERS FROM TEACHER TO GET IT TO WORK

### Continuous predictors are linearly related to the $logit(\pi)$

Logistic regression models assume linear relationship between predictor variables and the logit of the outcome variable. This assumption is mainly concerned with continuous predictors. Since we only have one continuous predictor we can plot the relationship between age and the logit of survived.

Our continuous predictors are `Age`, `Glucose` and `DiabetesPedigreeFunction`. We plot the relationships of these predictors and the logit of `Outcome`.

```{r}
# making the logit
dataRDS$logit <- predict(fit2, type="link")

# plotting the relations
dataRDS %>%
  ggplot(aes(Age, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()

dataRDS %>%
  ggplot(aes(Glucose, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()

dataRDS %>%
  ggplot(aes(DiabetesPedigreeFunction, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()
```

Age, Glucose and DiabetesPedigreeFunction all appear to not be linearly related to the predicted logit values. To try and fix this, we have to apply transformations to the variables.

```{r}
# transformations
dataRDS$AgeTransformed <- dataRDS$Age^0.1

# plotting the relations
dataRDS %>%
  ggplot(aes(AgeTransformed1, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()
```

remove outlier:

```{r}
plot(fit2, 4)

cooks <- cooks.distance(fit2)
cutoff <- 10 / nrow(dataRDS)
outliers <- which(cooks > cutoff)

dataRDS[outliers,]

dataRDS <- dataRDS[-outliers,]

fit2 <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction,  
            data = dataRDS)
plot(fit2, 4)
```

**Exploring the model**

```{r}
nullFit <- glm(Outcome ~ 1, family = binomial, data = dataRDS)
## Test the fit of our example model:
anova(nullFit, fit2, test = "Chisq")

car::vif(fit2) # shows no big multicolinearity
plot(fit2)
plot(fit2, 4)


car::crPlots(fit2, "Glucose")
dfbetas(fit2)[ , "Glucose"] |> plot()

# read performance
library(dplyr)
## Add predictions to the dataset:
dataRDS <- dataRDS %>%
  mutate(
    piHat = predict(fit2, type = "response"),  # predicted probabilities
    yHat  = factor(ifelse(piHat > 0.5, 1, 0), levels = c(0, 1))
  )

# Make levels the same
dataRDS$y <- factor(dataRDS$y, levels = c(0, 1))

library(caret)
cMat <- confusionMatrix(
  data = dataRDS$yHat,
  reference = dataRDS$y
)

cMat$table
cMat$byClass

# ROC Curve
rocData <- dataRDS %>%
pROC::roc(Outcome, piHat)
plot(rocData)

#CEE for comparison good models
```

## **Conclusion**

Cijfers over diabetes. (n.d.). diabetes.nl. https://www.diabetes.nl/wat-is-diabetes/over-diabetes/cijfers- over-diabetes

Umcu. (n.d.). Diabetes. UMC Utrecht. https://www.umcutrecht.nl/nl/ziekte/diabetes

Akturk, M. & National Institute of Diabetes and Digestive and Kidney Diseases. (2020). Diabetes \[Dataset\]. https://www.kaggle.com/datasets/mathchi/diabetes-data-set
