---
title: "Predicting Diabetes"
authors:
- "Demi van den Biggelaar (9660089)"
- "August Gesthuizen (5292565)"
- "Friso Harff (7526946)"
- "Leander van der Waal (7180063)"
date: "2026-01-15"
format:
  html:
    embed-resources: true
---

# **Prediction of glucose using logistic regression**

Demi van den Biggelaar (9660089)\
August Gesthuizen (5292565)\
Friso Harff (7526946)\
Leander van der Waal (7180063)\

## **Introduction**

1 in 14 Dutch citizens has diabetes (Cijfers Over Diabetes, z.d.). To treat diabetes more effectively and to prevent serious complications it is crucial to diagnose it as early as possible. Having diabetes means that there is a shortage of insulin in the body, causing glucose to stay in the blood instead, which can cause a range of complications (Umcu, z.d.).

We want to see if we can predict whether someone has diabetes using logistic regression.

To see if this is possible we use a dataset about diabetes and we use the different parameters to verify whether we can predict diabetes (Akturk & National Institute of Diabetes and Digestive and Kidney Diseases, 2020). The dataset is filtered on only females of at least 21 years of age from a Pima Indian heritage.

The different parameters in the dataset are:

-   Pregnancies: Number of times pregnant

-   Glucose: Plasma glucose concentration after 2 hours in an oral glucose tolerance test

-   BloodPressure: Diastolic blood pressure (mm Hg)

-   SkinThickness: Triceps skin fold thickness (mm)

-   Insulin: 2-Hour serum insulin (mu U/ml)

-   BMI: Body mass index (weight in kg/(height in m)\^2)

-   DiabetesPedigreeFunction: Diabetes pedigree function

-   Age: Age (years)

-   Outcome: Class variable (0 or 1, where 1 means positive for diabetes)

Our research question is as follows: can we predict whether someone has diabetes based on the parameters given in the dataset, namely: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and outcome or a subset of these paramaters using logistic regression?

## **Preprocessing & Loading data**

To preprocess the data we categorized the BMI into four categories, being underweight, healthy, overweight and obese. We also remove entries where we see that values are clearly skewed, for example a BMI of 0 should not exist.

------------------------------------------------------------------------

```{r}
# Initialization
setwd(".")
library(dplyr)
library(ggplot2)
library(magrittr)
library(kableExtra)
library(pROC)
library(regclass)
library(caret)
```

```{r}
#| warning: false

# Load .CSV and save as .RDS

datacsv <- read.csv("./data/raw/diabetes.csv", header = TRUE)
saveRDS(datacsv, "./data/raw/dataset.rds")

# Preprocessing
dataRDS <- readRDS("./data/raw/dataset.rds")
dataRDS <- na.omit(dataRDS) # removes NA from important columns
saveRDS(dataRDS, "./data/processed/dataset.rds")
dataset <- readRDS("./data/processed/dataset.rds")

# Recoding BMI to classes
dataRDS$BMI[dataRDS$BMI <= 18.5 ] <- 1                     # Underweight
dataRDS$BMI[dataRDS$BMI > 18.5 & dataRDS$BMI <= 25  ] <- 2 # Healthy
dataRDS$BMI[dataRDS$BMI > 25 & dataRDS$BMI <= 30 ] <- 3    # Overweight
dataRDS$BMI[dataRDS$BMI > 30] <- 4                         # Obese

# Removing missing values
dataRDS <- subset(dataRDS,
                     BloodPressure != 0 &
                     SkinThickness != 0 &
                     Glucose != 0 &
                     Insulin != 0 &
                     BMI != 0)

summary(dataset)
```

## **Exploring the data**

```{r}
#| warning: false

dataRDS
for (i in 1:ncol(dataRDS)) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
pairs(dataRDS)
```

Above you can see multiple ways that our variables are described. They are represented in two ways: using histograms and using.a panel. In the histograms you can see which values are representative of each variable while in the panel you can see the relation between of each variable with each other variable. The histograms will be briefly discussed, however remaining discussion will focus only on the relevant plots from the panel which we use for regression.

-   **Pregnancies:** number of times pregnant, represented as an integer, values range between 0-14

-   **Glucose:** plasma glucose concentration in a toleration test represented as an integer, values range from 56-198.

-   **BloodPressure:** Bloodpressure represented as an integer with values in the range of 24-110.

-   **Skinthickness:** Thickness of the skin in millimetres, represented as an integer with values ranging from 7-63.

-   **Insulin:** “2 hour serum insulin” with values ranging from 14-846.

-   **BMI:** BMI value represented as classification, namely: underweight=1, healthy=2, overweight=3 and obese=4. Most people in this study are considered to be obese.

-   **DiabetesPedigreeFunction:** Quantified genetic risk of diabetes. Risks range from 0-1,75. 

-   **Age:** Integer representing age, values range from 20-70.

-   **Outcome:** 0 or 1 value representing if someone has diabetes or not. Most participants don’t have diabetes.

-   **Moderation:** Product of the following values: BMI, Insulin, SkinThickness and BloodPressure. This value is added to support the regression accuracy.

## **Building the model**

```{r}
fit0 <- glm(Outcome ~ Age + Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction,  
            family = binomial,
            data = dataRDS)

fit1 <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction,  
            family = binomial,
            data = dataRDS)

fit2 <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction + Glucose*BMI,
            family = binomial,
            data = dataRDS)

fit3 <- glm(Outcome ~ Glucose + BMI + DiabetesPedigreeFunction, 
            family = binomial,
            data = dataRDS)

fit4 <- glm(Outcome ~ Glucose + BMI,
            family = binomial,
            data = dataRDS)

fit_exploratory <- glm(Outcome ~ Age + Glucose + BMI + DiabetesPedigreeFunction + Age*Glucose,
                       family = binomial,
                       data = dataRDS)
# Compare AIC
AIC(fit1, fit2, fit_exploratory)

AIC(fit0)
BIC(fit0)

# fit1 is best
AIC(fit1)
BIC(fit1)
summary(fit1)

AIC(fit2)
BIC(fit2)
AIC(fit3)
BIC(fit3)
AIC(fit4)
BIC(fit4)
```

Model Comparison Procedure (including moderation)

We compared six different models to find the optimal balance between explanatory power and simplicity

We compared the models using 4 different methods

We started with estimating several logistic regression models based on the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), where lower values indicate a better trade-off between model fit and complexity.

Fit 0 (Full Model): Included all available predictors to establish a baseline. Although Fit 0 contains all available predictors, it resulted in a higher AIC and several non-significant coefficients. We identified that variables such as BloodPressure, SkinThickness, pregnancies and Insulin were not statistically significant (p \> 0.05).

Fit 1 (Selected Model): A reduced model containing only Age, Glucose, BMI, and DiabetesPedigreeFunction. All included predictors show a positive relationship with the outcome and are statistically significant (p \< 0.05). This means that an increase in any of these variables is associated with a higher probability of being diagnosed with diabetes. Additionally, fit1 has the lowest AIC/BIC score of all the models.

Interpertation: o Glucose: As the strongest predictor, higher plasma glucose levels significantly increase the odds of diabetes. o BMI: A higher Body Mass Index is essentially linked to a higher diabetes risk, independent of the glucose level (since the moderation was removed). o Age & Pedigree: Older age and a pedigree function also independently contribute to a higher risk.

Fit 2 (Moderation Model): We hypothesized that the effect of glucose on diabetes risk might be stronger for individuals with a higher BMI. To test this, we added an interaction term (moderation) to fit 1 between Glucose and BMI (Glucose \* BMI). The moderation was not statistically significant (p \> 0.05), suggesting that the effect of Glucose on the probability of diabetes is independent of BMI. So this isn’t the best model either because only more variables are added without a decrease in the AIC/BIC score.

Fit 3 & 4 (Simplified Models): Models with fewer predictors than fit 1 to test if excluding variables like Age in Fit 3, and both age and DiabetesPedigreeFunction in fit 4 would harm the model fit. As a result, the AIC/BIC increased, which means that these models are under-fitting: they are missing crucial information to make a good prediction.

Exploratory model: We also tested if the impact of glucose levels changes with age making a moderation between age and glucose (age\*glucose). Similar to the BMI interaction, adding this term increased the complexity of the model without providing a substantial improvement in model fit1

To select a reliable model, we look beyond just the AIC/BIC scores.

ANOVA We specifically use this to test whether adding extra complexity, such as moderation (interaction terms), statistically improves the model. It helps us determine if a simpler model (fit1) is sufficient or if a more complex one is required.

We tested whether the interaction between Glucose and BMI (fit2) or Age and Glucose (fit_exploratory) provided added value. The ANOVA test indicated that these moderations were not significant (p \> 0.05). Therefore, the extra complexity of moderation does not result in a better prediction, leading us to choose the simpler fit1 model

Confusion matrix Confusion Matrix: This table demonstrates how the model performs in practice by comparing predictions to actual outcomes. It provides insight into the accuracy, as well as how well the model identifies positive cases (sensitivity) versus negative cases (specificity).

ROC Curve The ROC curve visualizes the model's diagnostic ability. The AUC (Area Under the Curve) serves as a 'grade' for the model: the closer to 1, the better the model is at distinguishing between individuals with and without diabetes.

Comparison with Complex Models (Fit 2 & Exploratory) We generated Confusion Matrixes for the models containing moderation (interactions). We found that the Accuracy and AUC did not improve compared to Fit 1. This confirms that adding complex interactions like Glucose \* BMI does not help the model distinguish between diabetic and healthy individuals any better.

Comparison with Simple Models (Fit 3 & 4) When looking at the ROC curves, these simpler models show a lower AUC. Their Confusion Matrices reveal a drop in Sensitivity, meaning they fail to identify actual diabetes cases that fit1 successfully catches.

Comparisson with ggpairs-plot to support our choice This choise also aligns with our findings from the ggpairs plot. As shown in the pairs plot, variables such as BloodPressure and SkinThickness exhibited significant overlap between the groups with and without diabetes. Consequently, these variables offered little discriminatory power, whereas Glucose and BMI showed a much clearer separation.

Which model is best? We choose model 1 because it is the most efficient model. It contains only the statistically significant variables that truly matter (Age, Glucose, BMI, and Genetic Predisposition) and avoids unnecessary complexity according tot he AIC/BIC scores.

## **Assumptions**

### Binary dependent variable

The first assumption in a logistic regression is that the outcome should be binary and therefore follow a binomial distribution. This is easy to check: you just need to be sure that the outcome can only take one of two responses. In our case, the possible outcomes are:

-   Diabetes (coded 1)

-   Not diabetes (coded 0)

```{r}
# Mutating Outcome into factor (necessary for next part)
dataRDS$Outcome <- as.factor(dataRDS$Outcome)

dataRDS %>% 
  ggplot(aes(x = Outcome, fill = Outcome)) +
  geom_bar() +
  labs(x = "Outcome",
       y = "Count",
       title = "Distribution of the Outcome variable") +
  theme_bw()
# You can see that there are indeed only two outcomes for `Outcome`, so our outcome follows a binomial distribution.
```

### Balanced outcomes

When using logistic regression to make predictions, then the accuracy will be affected by imbalance in the outcome classes. Notice that in the plot we just made there are more people who do not have diabetes than who do. A possible consequence is reduced accuracy in classification of diabetes.

A certain amount of imbalance is expected and can be handled well by the model in most cases. The effects of this imbalance is context-dependable. Some solutions to serious class imbalance are down-sampling or weighting the outcomes to balance the importance placed on the outcomes by the model. In this case, we consider the imbalance to be not extreme enough to not be handled sufficiently by the model.

### Sufficiently large sample size

Sample size in logistic regression is a complex issue, but some suggest that it is ideal to have 10 cases per candidate predictor in your model. The minimum number of cases to include is $N=\frac{10k}{p}$, where $k$ is the number of predictors and $p$ is the smallest proportion of negative or positive cases in the population.

```{r}
#First we need to get the proportion of people that have diabetes in our sample, which is 0.33

dataRDS %>% 
  count(Outcome) %>% 
  mutate(prop = n / sum(n))
```

```{r}
#Now we can plug this into our formula to get the minimum number of positive cases. 
ssize_cal <- function(k, p){
  round((10*k)/p)
}
# using k=4 as we have 4 predictors and p=0.33 as that is the proportion of diabetes=1.
ssize_cal(4, 0.33)
```

As can be seen in the Distribution of the Outcome variable chart, we have clearly more than the required 121 positive cases. We have 130 positive cases for diabetes. This means that our sample size is large enough.

### Predictor matrix is full-rank

For logistic regression it is important that there need to be more observations than predictors and there should be no multicollinearity among the linear predictors.

```{r}
VIF(fit1)
```

All VIF scores are close to 1, meaning that they are not correlated. Therefore, this assumption is not violated.

### Continuous predictors are linearly related to the $logit(\pi)$

Logistic regression models assume linear relationship between predictor variables and the logit of the outcome variable. This assumption is mainly concerned with continuous predictors. Our continuous predictors are `Age`, `Glucose` and `DiabetesPedigreeFunction`. We plot the relationships of these predictors and the logit of `Outcome`.

```{r}
# making the logit
dataRDS$logit <- predict(fit1, type="link")

# plotting the relations
dataRDS %>%
  ggplot(aes(Age, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()

dataRDS %>%
  ggplot(aes(Glucose, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()

dataRDS %>%
  ggplot(aes(DiabetesPedigreeFunction, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "glm") +
  theme_bw()
```

In the plots above it is clear that the blue line follows the pattern in the data nicely. A critic might argue that, for example in the relation between age and the logit, the pattern dips a little around age=50 to age=60, but there is no clear curved relation present. Therefore, this is not really a problem and the assumption is not violated.

### No influential values or outliers

Influential values are extreme individual data points that can affect the fit for the logistic regression model. They can be visualised using Cook's distance and the Residuals vs Leverage plot.

The Cook's distance plot shows the top 3 largest values. Not all outliers are influential cases, so the Residuals vs Leverage plot should be inspected as well, which shows actual influential cases. Leverage is the extend that the model coefficients will change if a particular observation is removed from the dataset.

```{r}
plot(fit1, c(4,5))
```

Points that fall outside of the dashed line (Cook's distance) are influential. There are no values which fall outside of the dashed lines, but to be sure we still look at what removing the most influential cases does.

```{r}
mostInf <- which(cooks.distance(fit1) > 0.04823)
dataRDS[mostInf, ]
dataRDS2 <- dataRDS[-mostInf, ]
glmFit2 <- update(fit1, data = dataRDS2)

```

```{r}

summary(fit1)$coef |> round(3)
summary(glmFit2)$coef |> round(3)

summary(fit1)
summary(glmFit2)
```

We see that removing the three most influential cases.

**Exploring the model**

```{r}
nullFit <- glm(Outcome ~ 1, family = binomial, data = dataRDS)
## Test the fit of our example model:
anova(nullfit, fit2, test = "Chisq")
anova(nullFit, fit_exploratory, test = "Chisq")

car::vif(fit2) # shows no big multicolinearity
plot(fit2)
plot(fit2, 4)


car::crPlots(fit2, "Glucose")
dfbetas(fit2)[ , "Glucose"] |> plot()

# read performance
library(dplyr)
## Add predictions to the dataset:
dataRDS <- dataRDS %>%
  mutate(
    piHat = predict(fit2, type = "response"),  # predicted probabilities
    yHat  = factor(ifelse(piHat > 0.5, 1, 0), levels = c(0, 1))
  )

# Make levels the same
dataRDS$y <- factor(dataRDS$y, levels = c(0, 1))

library(caret)
cMat <- confusionMatrix(
  data = dataRDS$yHat,
  reference = dataRDS$y
)

cMat$table
cMat$byClass

# ROC Curve
rocData <- dataRDS %>%
pROC::roc(Outcome, piHat)
plot(rocData)

#CEE for comparison good models
```

## **Interpretation**

### Coefficients table

For our final model we chose fit1 since this one best aligns to explain the outcome variable. The outcome (diabetes) is coded as 0 = no diabetes and 1 = diabetes.

```{r}
# Generating coefficients table for fit1 model
summary(fit1)

# Generating odds ratios
coef(fit1) |> exp()
```

### Coefficients and odds ratio analysis

The intercept was found to differ significantly from zero, showing the baseline log-odds of an individual having diabetes to be equal to -11.102 with a corresponding odds ratio of 1.51e-05 (β = -11.102, z = -8.674, p \< 2e-16). Age shows a value of .051 log-odds with an odds ratio of 1.052 or a 5,2% increase in the odds of having diabetes for every increasing value of age (β = .051, z = .013, p \< .000). For glucose, the log-odds are .036 and the odds ratio 1.036 or a 3.6% increase in the odds of having diabetes for every increasing value of glucose (β = .036, z = .005, p \< 1.26e-12). BMI shows a log-odds of .999 with a corresponding odds ratio of 2.715 or a 171,5% increase in the odds of having diabetes for every increasing value of BMI (β = .999, z = .259, p \< .000). ***With BMI it is different since it is not a continuous variable, and thus the 171,5% increase counts for the increase in BMI category.*** Diabetes pedigree function shows a log-odds of 1.129 and an odds ratio of 3.093 or a 209.3% increase in the odds of having diabetes for every increasing value of diabetes pedigree function (β = 1.129, z = 3.093, p \< .006). All coefficients are significant (p \< .05) with BMI and Diabetes pedigree function showing the highest percentage increase. This however may be due to the range of values in the data set seeing as ***BMI has only 4 categories*** and Diabetes pedigree function has a range of .08 to 2.42, which can also explain the larger odds ratio values compared to the other variables.

### Confusion matrix

To look at the efficiency of predictions of our model, a confusion matrix was printed and evaluated.

```{r}
## Generating confusion matrix
cMat <- dataRDS %$% confusionMatrix(data = yHat, reference = Outcome)

cMat$table
cMat$overall
cMat$byClass

TP <- 76
TN <- 234
FP <- 29
FN <- 54
P <- (TP + FN)
N <- (TN + FP)

# Calculating accuracy
((TP + TN) / (P + N)) * 100

# Calculating error rate
(1 - (TP + TN) / (P + N)) * 100

# Calculating sensitivity
(TP / P) * 100

# Calculating specificity
(TN / N) * 100

# Calculating false positive rate (FPR)
(1 - (TN / N)) * 100

# Calculating positive predictive value (PPV)
(TP / (TP + FP)) * 100

# Calculating negative predicitve value (NPV)
(TN / (TN + FN)) * 100

# Checking the ROC curve and calculating AUC value
rocData <- dataRDS %$%
pROC::roc(Outcome, piHat)
plot(rocData)

pROC::auc(rocData)
```

From the confusion matrix a few conclusions can be made. First of all, with a total of 393 individuals participating in the data set, 263 fall under the negative outcome and thus have no diabetes, 130 individuals fall under the positive outcome and thus do have diabetes. 234 of those 263 were correctly estimated to not have diabetes. This results in a **specificity** value of 89% meaning that 89% of individuals without diabetes are correctly classified. 76 out of the 130 were correctly estimated to have diabetes. This resulted in a **sensitivity** value of 58,5%, thus 58,5% of the individuals were correctly classified to have diabetes. Overall, the confusion matrix shows results with an **accuracy** of 78.9% (thus 78.9% of all cases were correctly classified), and an error rate of 21.1% (thus 21.1% of individuals were incorrectly classified).

11% is incorrectly classified as having diabetes while in reality this is not the case (type I error), there is a 72,4% that a person with diabetes is classified correctly, and a 81,3% chance that a person is correctly classified as not having diabetes. For application of this model it is better used to diagnose people without diabetes since the correct rate for diagnoses is 81,3% compared to the correct diagnoses rate of diabetes, which is 72,4%.

The AUC value is .8618 which falls within the range of 0.8 - 0.9 and is considered an excellent fit (Mandrekar, 2010). This is to be expected since the variables we chose to put into the model were expected to best predict the outcome variable.

## **Conclusion**

## Literature

Cijfers over diabetes. (n.d.). diabetes.nl. https://www.diabetes.nl/wat-is-diabetes/over-diabetes/cijfers- over-diabetes

Umcu. (n.d.). Diabetes. UMC Utrecht. https://www.umcutrecht.nl/nl/ziekte/diabetes

Akturk, M. & National Institute of Diabetes and Digestive and Kidney Diseases. (2020). Diabetes \[Dataset\]. https://www.kaggle.com/datasets/mathchi/diabetes-data-set

Mandrekar, J. N. (2010). Receiver operating characteristic curve in diagnostic test assessment. Journal of Thoracic Oncology, 5(9), 1315–1316.
