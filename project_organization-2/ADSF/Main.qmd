---
title: "Predicting Glucose"
authors:
- "Demi van den Biggelaar (9660089)"
- "August Gesthuizen (5292565)"
- "Friso Harff (7526946)"
- "Leander van der Waal (7180063)"
date: "2026-01-24"
format:
  html:
    embed-resources: true
---

**Introduction**

1 in 14 Dutch citizens has diabetes (Cijfers Over Diabetes, z.d.). To treat diabetes more effectively and to prevent serious complications it is crucial to diagnose it as early as possible. Having diabetes means that there is a shortage of insulin in the body, causing glucose to stay in the blood instead, which can cause a range of complications (Umcu, z.d.). Diabetes can be diagnosed by measuring the plasma glucose concentration in the blood (Diabetes Mellitus Type 2 \| LUMC, z.d.). To diagnose diabetes faster we want to know if we can predict glucose levels based on other parameters.

To see if this is possible we use a dataset about diabetes and we use the different parameters to verify whether we can predict glucose (Akturk & National Institute of Diabetes and Digestive and Kidney Diseases, 2020). The data is filtered on only females of at least 21 years of age from a Pima Indian heritage.

The different parameters in the dataset are:

-   Pregnancies: Number of times pregnant

-   Glucose: Plasma glucose concentration after 2 hours in an oral glucose tolerance test

-   BloodPressure: Diastolic blood pressure (mm Hg)

-   SkinThickness: Triceps skin fold thickness (mm)

-   Insulin: 2-Hour serum insulin (mu U/ml)

-   BMI: Body mass index (weight in kg/(height in m)\^2)

-   DiabetesPedigreeFunction: Diabetes pedigree function

-   Age: Age (years)

-   Outcome: Class variable (0 or 1, where 1 means positive for diabetes)

Our research question is as follows: can we predict whether someone has diabetes based on the parameters given in the dataset, namely: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and outcome or a subset of these paramaters using logistic regression?

## **Preprocessing & Loading data**

To preprocess the data we categorized the BMI into four categories, being underweight, healthy, overweight and obese. We have kept all the data, because there were not many missing values or flukes in the data.

------------------------------------------------------------------------

```{r}
#| warning: false

# Initialization
setwd(".")
library(ggplot2)
library(car)
library(magrittr)
library(regclass)
library(irr)
library(lmtest)
library(sandwich)
```

```{r}
#| warning: false

# Load .CSV and save as .RDS

datacsv <- read.csv("./data/raw/diabetes.csv", header = TRUE)
saveRDS(datacsv, "./data/raw/dataset.rds")

# Preprocessing
dataRDS <- readRDS("./data/raw/dataset.rds")
dataRDS <- na.omit(dataRDS) # removes NA from important columns
saveRDS(dataRDS, "./data/processed/dataset.rds")
dataset <- readRDS("./data/processed/dataset.rds")

# Recoding BMI to classes
dataRDS$BMI[dataRDS$BMI <= 18.5 ] <- 1
dataRDS$BMI[dataRDS$BMI > 18.5 & dataRDS$BMI <= 25  ] <- 2
dataRDS$BMI[dataRDS$BMI > 25 & dataRDS$BMI <= 30 ] <- 3
dataRDS$BMI[dataRDS$BMI > 30] <- 4

# Removing missing values
dataRDS <- subset(dataRDS,
                     Glucose != 0 &
                     Insulin != 0 &
                     BMI != 0 &
                     SkinThickness != 0)

```

## **Exploring the data**
In the following section we will explore each part of the dataset to get a full understanding of our dataset. We do this using a range of histograms that show the distribution of each value with an interpretation of why that data matters.

```{r}
#| warning: false
#| echo: false
for (i in 1:1) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "pink",
       border = "white")
}
```
-   **Pregnancies:** number of times pregnant, represented as an integer, values range between 0-14

Pregnancies is a fairly uninteresting distribution, only a couple people get pregnant more than a few times. Which makes sense! What is good to know is that pregnancies is highly influenced by age, because young people usually have not been pregnant often

```{r}
#| echo: false
for (i in 2:2) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "green",
       border = "white")
}
```
-   **Glucose:** plasma glucose concentration in a toleration test represented as an integer, values range from 56-198.

There is not a lot to say about the glucose distribution, it maintains the above stated values with a seemingly normal distribution that fits within norms of a healthy person.
```{r}
#| echo: false
for (i in 3:3) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "red",
       border = "white")
}
```
-   **BloodPressure:** Bloodpressure represented as an integer with values in the range of 24-110.

Bloodpressure is more interesting! It ranges wide enough to question the validity of some datapoints, namely some values of bloodpressure are too low to sustain a healthy person. Meaning some of the data we use can be flawed or perhaps taken from patients in intensive care situations.

```{r}
#| echo: false
for (i in 4:4) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "beige",
       border = "white")
}
```
-   **Skinthickness:** Thickness of the skin in millimetres, represented as an integer with values ranging from 7-63.

Skinthickness represents the thickness of the skin under the upper arm, which ranges in a normal distribution, no inferences are made using this graph so not a lot can be said about its value.
```{r}
#| echo: false
for (i in 5:5) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
```
-   **Insulin:** ‚Äú2 hour serum insulin‚Äù with values ranging from 14-846.

Insulin is a very important variable measured in units per milliliter with a wide range including absurdly high values. Values ranging from 20-200 can be reasonably explained but the same cannot be said about values 800+. We assume this data could be invalid or taken right after an insulin injection which is why these datapoints are not yet removed.
```{r}
#| echo: false
for (i in 6:6) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "orange",
       border = "white")
}
```
-   **BMI:** BMI value represented as classification, namely: underweight=1, healthy=2, overweight=3 and obese=4. Most people in this study are considered to be obese.

The most interesting part of this data is that 259/393 = 66% are obese which means our model will most likely not be good at predicting glucose for people who are underweight and less good for healthy and overweight people. While this model will be the best for predicting glucose in severely overweight people. Keep this in mind when we interpret our model.

```{r}
#| echo: false
#| warning: false
for (i in 7:7) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}

```
-   **DiabetesPedigreeFunction:** Quantified genetic risk of diabetes. Risks range from 0-2,24.

The diabetes pedigree function is a function that predicts diabetes using hereditary information, values in this dataset conform to the limits of the function and range normally on lower values with some extremer values on the high end of the values. Due to this fitting within function norms there is no reason to assume this data is invalid.

```{r}
#| echo: false
for (i in 8:8) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "black",
       border = "white")
}

```
-   **Age:** Integer representing age, values range from 21-81.

Age ranges widely in our dataset, it largely ranges between 21 and 60 with only a couple values above 60. Keep this in mind while interpreting our model for this means the model is not properly trained on anyone under 21 and over 60. Meaning these are the ages we likely want to use our model on.

```{r}
#| echo: false
#| warning: false
for (i in 9:9) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
```
-   **Outcome:** 0 or 1 value representing if someone has diabetes or not.

Most participants don‚Äôt have diabetes in our model but values are fairly balanced by about a 2:1 split, however because we are trying to predict glucose to help people find diabetes earlier in their lifetime we will not be using this value in our model.

```{r}
#| echo: false
ggpairs(dataRDS)

```
In this pairs plot you can see how each value interacts with each other. Most predictors do not strongly correlate with each other with the exception of a few, namely pregnancies and age, glucose and insulin or glucose and outcome. These values are logically very closely related, for example the older you are the more pregnancies you can have. Insulin helps process glucose which directly influences it's values. A higher percentage of glucose is expected when you suffer from diabetes.


## **Building the model**

```{r}
#| warning: false


# List of all models we could think of.
models <- list(
  
   # model with all predictor p < 0.05 
   mod2 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness + DiabetesPedigreeFunction, 
                data = dataRDS),
   # model with all predictors p < 0.01 and moderation
  mod3 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness , 
              data = dataRDS),
  
  mod4 = lm(Glucose ~ Insulin + BMI + SkinThickness + Age*Pregnancies, 
              data = dataRDS),
  # model with all predictors p < 0.01 and moderation
  mod6 = lm(Glucose ~ Age + Insulin + BMI, 
                data = dataRDS),
  m1_full =
    lm(Glucose ~ Age + Insulin + BMI + Pregnancies +
         BloodPressure + SkinThickness + DiabetesPedigreeFunction,
       data = dataRDS),
  
  m2_reduced =
    lm(Glucose ~ Age + Insulin + BMI +
         SkinThickness + DiabetesPedigreeFunction,
       data = dataRDS),
  
  m3_core =
    lm(Glucose ~ Age + Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m4_metabolic =
    lm(Glucose ~ Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m5_simple =
    lm(Glucose ~ Insulin + BMI,
       data = dataRDS),
  
  m6_minimal =
    lm(Glucose ~ BMI,
       data = dataRDS),
  
  m7_age_preg =
    lm(Glucose ~ Age * Pregnancies +
         Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m8_insulin_bmi =
    lm(Glucose ~ Insulin * BMI +
         Age + SkinThickness,
       data = dataRDS),
  
  m9_insulin_skin =
    lm(Glucose ~ Insulin * SkinThickness +
         Age + BMI,
       data = dataRDS),
  
  m10_bmi_skin =
    lm(Glucose ~ BMI * SkinThickness +
         Age + Insulin,
       data = dataRDS),
  
  m11_genetic_bmi =
    lm(Glucose ~ DiabetesPedigreeFunction * BMI +
         Age + Insulin + SkinThickness,
       data = dataRDS),
  
  model4 = lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
             data = dataRDS),
  model35 =lm(Glucose ~ Insulin * Age + BloodPressure, data = dataRDS), # AIC=3612.325, BIC=3636.168 -> best
  model36 =lm(Glucose ~ Insulin * Age + BloodPressure + BMI, data = dataRDS) 
)


# Makes a dataframe of all scores in the model list.
comparison <- data.frame(
  Model = names(models),
  AIC   = sapply(models, AIC),
  BIC   = sapply(models, BIC),
  Adj_R2 = sapply(models, function(m) summary(m)$adj.r.squared)
) 

comparison[order(comparison$AIC), ]

model3 <- lm(Glucose ~ Insulin * Age + BloodPressure, data = dataRDS)
model4 <- lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
             data = dataRDS)
# Plot with regression line for x1
ggplot(dataRDS, aes(x = Age, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # se = FALSE to hide confidence interval
  labs(title = "Linear Relationship between Age and Glucose")

# Plot with regression line for x2
ggplot(dataRDS, aes(x = Insulin, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Linear Relationship between Insulin and Glucose")

# Plot with regression line for x1
ggplot(dataRDS, aes(x = SkinThickness, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # se = FALSE to hide confidence interval
  labs(title = "Linear Relationship between Skin thickness and Glucose")

# Plot with regression line for x2
ggplot(dataRDS, aes(x = BMI, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Linear Relationship between BMI and Glucose")




```

#################DISCUSSIE OVER MODEL BOUWEN EN ONDERBOUWEN WAAROM HET GOED IS, DIT GECOMBINEERD MET EEN BETEKENISVOLLE MODERATION TOEVOEGEN##########

## **Assumptions**

We will now discuss and check the various model assumptions of linear regression.

### Linearity

With the assumption of linearity, it is assumed that the relation between the dependent and independent variables is more or less linear. If this assumption would be violated, we could not be doing a linear regression and we should move to an entirely different modeling paradigm.

Because it is multiple regression, a basic residual plot is not sufficient as it would not tell us which predictors exhibit nonlinear associations. Instead, Component + Residual Plots or partial residual plots can be used to visualize the unique effects of each predictor.

```{r}
crPlots(model4)
```

In the Component + Residual Plots it is clear that all predictors seem to have a (near) linear relationship with the Component+Residual. For Insulin there seems to be a kink in the otherwise linear relationship. We consider this as sufficient to not violate the linearity assumption.

### Predictor matrix is full rank

This assumption states that there need to be more observations than predictors and no predictor can be a linear combination of other predictors, meaning predictors cannot have a very high correlation (multicollinearity). If this assumption were to be violated, the parameter estimates could not be uniquely determined from the data.

To check for this assumption, we use the Variance Inflation Factor (VIF) to determine multicollinearity.

```{r}
VIF(model4)
```

A VIF-score should be interpreted as follows:

-   VIF=1: this means no multicollinearity;

-   VIF=5: this implies already quite some multicollinearity;

-   VIF=10: this implies problematic amounts of multicollinearity.

Because the VIF-scores for all four predictors is near 1, this assumption is not violated.

### Exogenous predictors

For this assumption, the expected value of the errors must be 0. Furthermore, the errors must be independent of the predictors. This entails that:

-   $Cov(\hat Y,\epsilon)=0$

-   $E[\epsilon_n]=0$

Because this is more a model assumption issue rather than something to directly test as the error term is unobserved. We assume the predictors not to be endogenous. Not meeting this assumption results in biased estimates for the regression coefficients.

### Constant, finite error variance

This assumption is the assumption of homoscedasticity. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values.

If this assumption would be violated, the estimated standard errors would be biased (usually downward). This would entail that test statistics would be too large, CIs wouldbe too narrow and we would have inflated Type I error rates. The heteroscedasticity will not bias the parameter estimates, meaning the best fit line would still be correct but our measure of uncertainty around that best fit line would be wrong.

```{r}
model4 %>%
  plot(1)
```

In the plot it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. This indicates the this assumption is not violated.

### Independent errors

This assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.

Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).

Our model does not seem to contain predictors that are measured over time, therefore we assume this to not be the case.

We check for clustering using the ICC scores.

```{r}
ICC::ICCbare(x=dataRDS$Pregnancies, y=resid(model4))
ICC::ICCbare(x=dataRDS$SkinThickness, y=resid(model4))
ICC::ICCbare(x=dataRDS$DiabetesPedigreeFunction, y=resid(model4))
```

For Pregnancies and Skin thickness, we see negligible clustering with an ICC between 0 and 0.05. For the diabetes pedigree function we see an ICC between 0.05 and 0.10, which indicates small clustering. With this, we conclude that this assumption is not violated.

### Normally distributed errors

This assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked using model plots.

```{r}
model4 %>%
  plot(2)
```

The Q-Q plot shows that the error terms follow the ideal line pretty well, thus the assumption holds.

If the assumption would have been violated, it would imply a misspecified model. Then the justification for some tests and procedures used in regression analysis might not hold. For prediction intervals, normality of errors is always desired.

------------------------------------------------------------------------

## Influential Observations

### Outliers

Outliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the externally studentized residuals.

```{r}
model4 %>%
  rstudent() %>%
  plot()
```

There does not appear to be a clear outlier in this plot.

### High-leverage observations

High-leverage observations are observations with extreme predictor values. To detect these observations, we look at their leverage values. These values are be summarized in a leverage plot.

```{r}
model4 %>%
  hatvalues() %>%
  plot()

# check highest leverage cases
h <- hatvalues(model4)
n <- nobs(model4)
p <- length(coef(model4))
cutoff <- 3*p/n
idx <- which(h > cutoff)
idx
```

Here we see a few possible outstanding observations, like the observation close to the beginning, two observations around 100 and an observation somewhere between observation 200 and 300. Looking in the dataset, these outstanding observations can be explained by their high values for Insulin. For example, index 5 has an insulin of 846, which is the largest of the dataset.

A case with high leverage is not necessarily bad, the influence on the model is more important.

### Influence on the model

Both outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem.

Influence measures come in two sorts: Cook's distance checks for influential observations, while DFBETAS check for influential, and possibly problematic, observations per regression coefficient.

First we check the Cook's distance.

```{r}
model4 %>%
  cooks.distance() %>%
  plot()
```

Here we see outstanding distances around the beginning and around 300. Next we check the DFBETAS.

```{r}
plot(dfbetas(model4)[,1],main="intercept")
plot(dfbetas(model4)[,2],main="slope")
```

In the DFBETAS plots, we see that different points are most outstanding, also different points as in the Cook's distance plot. From this we conclude that there is no reason to assume they are problematic.

# EINDE van Assumptions en outliers/influential cases

The other four assumptions of the linear regression model that were checked are linearity, homoscedasticity, normal distribution, and correlated errors. The Q-Q plot and density plot analysis suggested a potential risk for violation of the normality assumption of residuals. The can be seen in the Q-Q plot by the upper tail deviating from the dotted line and in the density plot by a slight skewing on the right of the distribution. A Shapiro-Wilk test was performed on the residuals which resulted in the test statistic W = 0.97, *p \< .05*. Normality assumption of residuals was thus not rejected. Homoscedasticity was assessed by plotting the residuals. Visual inspection of the residuals suggested heteroscedasticity with an increase in spread of the residuals at higher predicted values. The Durbin-Watson test was run to look for autocorrelation of the errors and showed a test statistic of DW ‚âà 2, indicating no autocorrelation in the residuals. Finally, for the correlation of errors, we ran individual visual analyses for every predicting variable. Age, insulin, and skin thickness all suggest no correlation. BMI, however, visibly increased together with the errors. This could be explained by BMI not being a continuous variable, the grouping of the variable resulting in an uneven distribution of data points, and thus showing a correlation with the errors.

\########## EXPAND on the effect of violating an assumption \############

## **Exploring the model**

```{r}
#| warning: false

summary(model4)
```

A multiple linear regression was conducted to examine the predictors for glucose levels. This was done through the use of age, insulin, BMI, and blood pressure. The overall model was significant (F(4, 388) = 64.49, p \> .01) and explains 39.94% of the variance in glucose (R¬≤ = .3994). Age has a positive effect on glucose levels where older individuals have a higher glucose level (ùõΩ = .59, p \< .01). The same is true for insulin levels (ùõΩ = .13, p \< .01) where insulin increases when glucose levels go up. Although not as significant as age and insulin, blood pressure shows a significant effect on the increase of glucose levels (ùõΩ = .21, p \< .05). BMI (ùõΩ = 2.72, p = .13) shows a positive effect, but not a significant one. We still kept BMI in the model to explain glucose levels, since, from a logical standpoint, glucose levels are linked to BMI. Overall, the results suggest that only age, insulin, and blood pressure contribute significantly to glucose level variation.

## **Conclusion**

To answer our research question, the results indicate that glucose levels can be predicted by age, insulin, and blood pressure which all showed a positive relation with glucose: as age, insulin, and blood pressure increase, glucose levels will also increase. BMI did not significantly predict glucose. This highlights that, within this sample, glucose levels are in part accounted for by age, insulin levels, and blood pressure rather than body composition.

Cijfers over diabetes. (n.d.). diabetes.nl. https://www.diabetes.nl/wat-is-diabetes/over-diabetes/cijfers- over-diabetes

Diabetes Mellitus type 2 \| LUMC. (n.d.). https://www.lumc.nl/afdelingen/endocrinologie/ziektes-en-aandoeningen/diabetes-mellitus-type-2/

Umcu. (n.d.). Diabetes. UMC Utrecht. https://www.umcutrecht.nl/nl/ziekte/diabetes

Akturk, M. & National Institute of Diabetes and Digestive and Kidney Diseases. (2020). Diabetes \[Dataset\]. https://www.kaggle.com/datasets/mathchi/diabetes-data-set
