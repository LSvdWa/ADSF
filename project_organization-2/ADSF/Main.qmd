---
title: "Predicting Glucose"
authors:
- "Demi van den Biggelaar (9660089)"
- "August Gesthuizen (5292565)"
- "Friso Harff (7526946)"
- "Leander van der Waal (7180063)"
date: "2026-01-24"
format:
  html:
    embed-resources: true
---

**Introduction**

1 in 14 Dutch citizens has diabetes (Cijfers Over Diabetes, z.d.). To treat diabetes more effectively and to prevent serious complications it is crucial to diagnose it as early as possible. Having diabetes means that there is a shortage of insulin in the body, causing glucose to stay in the blood instead, which can cause a range of complications (Umcu, z.d.). Diabetes can be diagnosed by measuring the plasma glucose concentration in the blood (Diabetes Mellitus Type 2 \| LUMC, z.d.). To diagnose diabetes faster we want to know if we can predict glucose levels based on other parameters.

To see if this is possible we use a dataset about diabetes and we use the different parameters to verify whether we can predict glucose (Akturk & National Institute of Diabetes and Digestive and Kidney Diseases, 2020). The data is filtered on only females of at least 21 years of age from a Pima Indian heritage.

The different parameters in the dataset are:

-   Pregnancies: Number of times pregnant

-   Glucose: Plasma glucose concentration after 2 hours in an oral glucose tolerance test

-   BloodPressure: Diastolic blood pressure (mm Hg)

-   SkinThickness: Triceps skin fold thickness (mm)

-   Insulin: 2-Hour serum insulin (mu U/ml)

-   BMI: Body mass index (weight in kg/(height in m)\^2)

-   DiabetesPedigreeFunction: Diabetes pedigree function

-   Age: Age (years)

-   Outcome: Class variable (0 or 1, where 1 means positive for diabetes)

Our research question is as follows: can we predict whether someone has diabetes based on the parameters given in the dataset, namely: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and outcome or a subset of these paramaters using logistic regression?

## **Preprocessing & Loading data**

To preprocess the data we categorized the BMI into four categories, being underweight, healthy, overweight and obese. We have kept all the data, because there were not many missing values or flukes in the data.

------------------------------------------------------------------------

```{r}
#| warning: false

# Initialization
setwd(".")
library(ggplot2)
library(car)
library(magrittr)
library(regclass)
library(irr)
library(lmtest)
library(sandwich)
```

```{r}
#| warning: false

# Load .CSV and save as .RDS

datacsv <- read.csv("./data/raw/diabetes.csv", header = TRUE)
saveRDS(datacsv, "./data/raw/dataset.rds")

# Preprocessing
dataRDS <- readRDS("./data/raw/dataset.rds")
dataRDS <- na.omit(dataRDS) # removes NA from important columns
saveRDS(dataRDS, "./data/processed/dataset.rds")
dataset <- readRDS("./data/processed/dataset.rds")

# Recoding BMI to classes
dataRDS$BMI[dataRDS$BMI <= 18.5 ] <- 1
dataRDS$BMI[dataRDS$BMI > 18.5 & dataRDS$BMI <= 25  ] <- 2
dataRDS$BMI[dataRDS$BMI > 25 & dataRDS$BMI <= 30 ] <- 3
dataRDS$BMI[dataRDS$BMI > 30] <- 4

# Removing missing values
dataRDS <- subset(dataRDS,
                     Glucose != 0 &
                     Insulin != 0 &
                     BMI != 0 &
                     SkinThickness != 0)

```

## **Exploring the data**

```{r}
#| warning: false

for (i in 1:ncol(dataRDS)) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
pairs(dataRDS)
```

\########## DESCRIBE WHY EACH HISTOGRAM IS MEANINGFUL AND WHERE IN THE PAIRS YOU CAN SEE INTERESTING INTERACTIONS##############

Above you can see multiple ways that our variables are described. They are represented in two ways: using histograms and using.a panel. In the histograms you can see which values are representative of each variable while in the panel you can see the relation between of each variable with each other variable. The histograms will be briefly discussed, however remaining discussion will focus only on the relevant plots from the panel which we use for regression.

-   **Pregnancies:** number of times pregnant, represented as an integer, values range between 0-14

-   **Glucose:** plasma glucose concentration in a toleration test represented as an integer, values range from 56-198.

-   **BloodPressure:** Bloodpressure represented as an integer with values in the range of 24-110.

-   **Skinthickness:** Thickness of the skin in millimetres, represented as an integer with values ranging from 7-63.

-   **Insulin:** ‚Äú2 hour serum insulin‚Äù with values ranging from 14-846.

-   **BMI:** BMI value represented as classification, namely: underweight=1, healthy=2, overweight=3 and obese=4. Most people in this study are considered to be obese.

-   **DiabetesPedigreeFunction:** Quantified genetic risk of diabetes. Risks range from 0-1,75.¬†

-   **Age:** Integer representing age, values range from 20-70.

-   **Outcome:** 0 or 1 value representing if someone has diabetes or not. Most participants don‚Äôt have diabetes.

## **Building the model**

```{r}
#| warning: false


# List of all models we could think of.
models <- list(
  
   # model with all predictor p < 0.05 
   mod2 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness + DiabetesPedigreeFunction, 
                data = dataRDS),
   # model with all predictors p < 0.01 and moderation
  mod3 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness , 
              data = dataRDS),
  
  mod4 = lm(Glucose ~ Insulin + BMI + SkinThickness + Age*Pregnancies, 
              data = dataRDS),
  # model with all predictors p < 0.01 and moderation
  mod6 = lm(Glucose ~ Age + Insulin + BMI, 
                data = dataRDS),
  m1_full =
    lm(Glucose ~ Age + Insulin + BMI + Pregnancies +
         BloodPressure + SkinThickness + DiabetesPedigreeFunction,
       data = dataRDS),
  
  m2_reduced =
    lm(Glucose ~ Age + Insulin + BMI +
         SkinThickness + DiabetesPedigreeFunction,
       data = dataRDS),
  
  m3_core =
    lm(Glucose ~ Age + Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m4_metabolic =
    lm(Glucose ~ Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m5_simple =
    lm(Glucose ~ Insulin + BMI,
       data = dataRDS),
  
  m6_minimal =
    lm(Glucose ~ BMI,
       data = dataRDS),
  
  m7_age_preg =
    lm(Glucose ~ Age * Pregnancies +
         Insulin + BMI + SkinThickness,
       data = dataRDS),
  
  m8_insulin_bmi =
    lm(Glucose ~ Insulin * BMI +
         Age + SkinThickness,
       data = dataRDS),
  
  m9_insulin_skin =
    lm(Glucose ~ Insulin * SkinThickness +
         Age + BMI,
       data = dataRDS),
  
  m10_bmi_skin =
    lm(Glucose ~ BMI * SkinThickness +
         Age + Insulin,
       data = dataRDS),
  
  m11_genetic_bmi =
    lm(Glucose ~ DiabetesPedigreeFunction * BMI +
         Age + Insulin + SkinThickness,
       data = dataRDS),
  
  model4 = lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
             data = dataRDS),
  model35 =lm(Glucose ~ Insulin * Age + BloodPressure, data = dataRDS), # AIC=3612.325, BIC=3636.168 -> best
  model36 =lm(Glucose ~ Insulin * Age + BloodPressure + BMI, data = dataRDS) 
)


# Makes a dataframe of all scores in the model list.
comparison <- data.frame(
  Model = names(models),
  AIC   = sapply(models, AIC),
  BIC   = sapply(models, BIC),
  Adj_R2 = sapply(models, function(m) summary(m)$adj.r.squared)
) 

comparison[order(comparison$AIC), ]

model3 <- lm(Glucose ~ Insulin * Age + BloodPressure, data = dataRDS)
model4 <- lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
             data = dataRDS)
# Plot with regression line for x1
ggplot(dataRDS, aes(x = Age, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # se = FALSE to hide confidence interval
  labs(title = "Linear Relationship between Age and Glucose")

# Plot with regression line for x2
ggplot(dataRDS, aes(x = Insulin, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Linear Relationship between Insulin and Glucose")

# Plot with regression line for x1
ggplot(dataRDS, aes(x = SkinThickness, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # se = FALSE to hide confidence interval
  labs(title = "Linear Relationship between Skin thickness and Glucose")

# Plot with regression line for x2
ggplot(dataRDS, aes(x = BMI, y = Glucose)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Linear Relationship between BMI and Glucose")




```

#################DISCUSSIE OVER MODEL BOUWEN EN ONDERBOUWEN WAAROM HET GOED IS, DIT GECOMBINEERD MET EEN BETEKENISVOLLE MODERATION TOEVOEGEN##########

## **Assumptions**

We will now discuss and check the various model assumptions of linear regression.

### Linearity

With the assumption of linearity, it is assumed that the relation between the dependent and independent variables is more or less linear. If this assumption would be violated, we could not be doing a linear regression and we should move to an entirely different modeling paradigm.

Because it is multiple regression, a basic residual plot is not sufficient as it would not tell us which predictors exhibit nonlinear associations. Instead, Component + Residual Plots or partial residual plots can be used to visualize the unique effects of each predictor.

```{r}
crPlots(model4)
```

In the Component + Residual Plots it is clear that all predictors seem to have a (near) linear relationship with the Component+Residual. For Insulin there seems to be a kink in the otherwise linear relationship. We consider this as sufficient to not violate the linearity assumption.

### Predictor matrix is full rank

This assumption states that there need to be more observations than predictors and no predictor can be a linear combination of other predictors, meaning predictors cannot have a very high correlation (multicollinearity). If this assumption were to be violated, the parameter estimates could not be uniquely determined from the data.

To check for this assumption, we use the Variance Inflation Factor (VIF) to determine multicollinearity.

```{r}
VIF(model4)
```

A VIF-score should be interpreted as follows:

-   VIF=1: this means no multicollinearity;

-   VIF=5: this implies already quite some multicollinearity;

-   VIF=10: this implies problematic amounts of multicollinearity.

Because the VIF-scores for all four predictors is near 1, this assumption is not violated.

### Exogenous predictors

For this assumption, the expected value of the errors must be 0. Furthermore, the errors must be independent of the predictors. This entails that:

-   $Cov(\hat Y,\epsilon)=0$

-   $E[\epsilon_n]=0$

Because this is more a model assumption issue rather than something to directly test as the error term is unobserved. We assume the predictors not to be endogenous. Not meeting this assumption results in biased estimates for the regression coefficients.

### Constant, finite error variance

This assumption is the assumption of homoscedasticity. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values.

If this assumption would be violated, the estimated standard errors would be biased (usually downward). This would entail that test statistics would be too large, CIs wouldbe too narrow and we would have inflated Type I error rates. The heteroscedasticity will not bias the parameter estimates, meaning the best fit line would still be correct but our measure of uncertainty around that best fit line would be wrong.

```{r}
model4 %>%
  plot(1)
```

In the plot it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. This indicates the this assumption is not violated.

### Independent errors

This assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.

Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).

Our model does not seem to contain predictors that are measured over time, therefore we assume this to not be the case.

We check for clustering using the ICC scores.

```{r}
ICC::ICCbare(x=dataRDS$Pregnancies, y=resid(model4))
ICC::ICCbare(x=dataRDS$SkinThickness, y=resid(model4))
ICC::ICCbare(x=dataRDS$DiabetesPedigreeFunction, y=resid(model4))
```

For Pregnancies and Skin thickness, we see negligible clustering with an ICC between 0 and 0.05. For the diabetes pedigree function we see an ICC between 0.05 and 0.10, which indicates small clustering. With this, we conclude that this assumption is not violated.

### Normally distributed errors

This assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked using model plots.

```{r}
model4 %>%
  plot(2)
```

The Q-Q plot shows that the error terms follow the ideal line pretty well, thus the assumption holds.

If the assumption would have been violated, it would imply a misspecified model. Then the justification for some tests and procedures used in regression analysis might not hold. For prediction intervals, normality of errors is always desired.

------------------------------------------------------------------------

## Influential Observations

### Outliers

Outliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the externally studentized residuals.

```{r}
model4 %>%
  rstudent() %>%
  plot()
```

There does not appear to be a clear outlier in this plot.

### High-leverage observations

High-leverage observations are observations with extreme predictor values. To detect these observations, we look at their leverage values. These values are be summarized in a leverage plot.

```{r}
model4 %>%
  hatvalues() %>%
  plot()

# check highest leverage cases
h <- hatvalues(model4)
n <- nobs(model4)
p <- length(coef(model4))
cutoff <- 3*p/n
idx <- which(h > cutoff)
idx
```

Here we see a few possible outstanding observations, like the observation close to the beginning, two observations around 100 and an observation somewhere between observation 200 and 300. Looking in the dataset, these outstanding observations can be explained by their high values for Insulin. For example, index 5 has an insulin of 846, which is the largest of the dataset.

A case with high leverage is not necessarily bad, the influence on the model is more important.

### Influence on the model

Both outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem.

Influence measures come in two sorts: Cook's distance checks for influential observations, while DFBETAS check for influential, and possibly problematic, observations per regression coefficient.

First we check the Cook's distance.

```{r}
model4 %>%
  cooks.distance() %>%
  plot()
```

Here we see outstanding distances around the beginning and around 300. Next we check the DFBETAS.

```{r}
plot(dfbetas(model4)[,1],main="intercept")
plot(dfbetas(model4)[,2],main="slope")
```

In the DFBETAS plots, we see that different points are most outstanding, also different points as in the Cook's distance plot. From this we conclude that there is no reason to assume they are problematic.

# EINDE van Assumptions en outliers/influential cases

The other four assumptions of the linear regression model that were checked are linearity, homoscedasticity, normal distribution, and correlated errors. The Q-Q plot and density plot analysis suggested a potential risk for violation of the normality assumption of residuals. The can be seen in the Q-Q plot by the upper tail deviating from the dotted line and in the density plot by a slight skewing on the right of the distribution. A Shapiro-Wilk test was performed on the residuals which resulted in the test statistic W = 0.97, *p \< .05*. Normality assumption of residuals was thus not rejected. Homoscedasticity was assessed by plotting the residuals. Visual inspection of the residuals suggested heteroscedasticity with an increase in spread of the residuals at higher predicted values. The Durbin-Watson test was run to look for autocorrelation of the errors and showed a test statistic of DW ‚âà 2, indicating no autocorrelation in the residuals. Finally, for the correlation of errors, we ran individual visual analyses for every predicting variable. Age, insulin, and skin thickness all suggest no correlation. BMI, however, visibly increased together with the errors. This could be explained by BMI not being a continuous variable, the grouping of the variable resulting in an uneven distribution of data points, and thus showing a correlation with the errors.

\########## EXPAND on the effect of violating an assumption \############

## **Exploring the model**

```{r}
#| warning: false

summary(model3)
```

A multiple linear regression was conducted to examine the predictors for glucose levels. This was done through the use of age, insulin, BMI, skin thickness, and a moderation variable. The overall model was significant (F(5, 387) = 50.39, p \> .01) and explains 39.4% of the variance in glucose (R¬≤ = .3934). Age has a positive effect on glucose levels where older individuals have a higher glucose level (ùõΩ = .65, p \< .01). The same is true for insulin levels (ùõΩ = .12, p \< .01) where insulin increases when glucose increases. However, both BMI (ùõΩ = 2.55, p = .23) and skin thickness (ùõΩ = .042, p = .82), although showing a positive effect, are both not significant. The moderation effect is also insignificant (ùõΩ = 1.207e-06, p = .60). Overall, the results suggest that only age and insulin contribute meaningfully to glucose level variation.

In model 3, an ANOVA was done to look at the significance of age, insulin, BMI, and skin thickness on glucose levels, and whether this association was influenced by the moderation term. The model showed that the interaction term does not influence the relationship between the predictors and glucose levels (F(1, 387) = 1.38, p \> .241).

## **Conclusion**

To answer our research question, the results indicate that glucose levels can be predicted by age and insulin, which both showed a positive relation with glucose: as age and insulin increase, glucose levels will also increase. BMI, skin thickness and the moderator did not predict glucose. This highlights that, within this sample, glucose levels are in part accounted for by insulin levels and age rather than body composition or the proposed moderation effect.

Cijfers over diabetes. (n.d.). diabetes.nl. https://www.diabetes.nl/wat-is-diabetes/over-diabetes/cijfers- over-diabetes

Diabetes Mellitus type 2 \| LUMC. (n.d.). https://www.lumc.nl/afdelingen/endocrinologie/ziektes-en-aandoeningen/diabetes-mellitus-type-2/

Umcu. (n.d.). Diabetes. UMC Utrecht. https://www.umcutrecht.nl/nl/ziekte/diabetes

Akturk, M. & National Institute of Diabetes and Digestive and Kidney Diseases. (2020). Diabetes \[Dataset\]. https://www.kaggle.com/datasets/mathchi/diabetes-data-set
