---
title: "Predicting Glucose"
authors:
- "Demi van den Biggelaar (9660089)"
- "August Gesthuizen (5292565)"
- "Friso Harff (7526946)"
- "Leander van der Waal (7180063)"
date: "2026-01-24"
format:
  html:
    embed-resources: true
---

**Introduction**

1 in 14 Dutch citizens has diabetes (Cijfers Over Diabetes, z.d.). To treat diabetes more effectively and to prevent serious complications it is crucial to diagnose it as early as possible. Having diabetes means that there is a shortage of insulin in the body, causing glucose to stay in the blood instead, which can cause a range of complications (Umcu, z.d.). Diabetes can be diagnosed by measuring the plasma glucose concentration in the blood (Diabetes Mellitus Type 2 \| LUMC, z.d.). To diagnose diabetes faster we want to know if we can predict glucose levels based on other parameters.

To see if this is possible we use a dataset about diabetes and we use the different parameters to verify whether we can predict glucose (Akturk & National Institute of Diabetes and Digestive and Kidney Diseases, 2020). The data is filtered on only females of at least 21 years of age from a Pima Indian heritage.

The different parameters in the dataset are:

-   Pregnancies: Number of times pregnant

-   Glucose: Plasma glucose concentration after 2 hours in an oral glucose tolerance test

-   BloodPressure: Diastolic blood pressure (mm Hg)

-   SkinThickness: Triceps skin fold thickness (mm)

-   Insulin: 2-Hour serum insulin (mu U/ml)

-   BMI: Body mass index (weight in kg/(height in m)\^2)

-   DiabetesPedigreeFunction: Diabetes pedigree function

-   Age: Age (years)

-   Outcome: Class variable (0 or 1, where 1 means positive for diabetes)

Our research question is as follows: can we predict whether someones glucose-concentration based on the parameters given in the dataset, namely: Pregnancies, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and outcome or a subset of these parameters using linear regression?

## **Preprocessing & Loading data**

To preprocess the data we categorized the BMI into four categories, being underweight, healthy, overweight and obese. We have not kept all the data, because there were missing values in the data. We removed these entries, as we had no way to get our hands on the fitting values.

------------------------------------------------------------------------

```{r}
#| warning: false
#| echo: false

# Initialization
setwd(".")
library(ggplot2)
library(car)
library(magrittr)
library(regclass)
library(irr)
library(lmtest)
library(sandwich)
```

```{r}
#| warning: false
#| echo: false

# Load .CSV and save as .RDS

datacsv <- read.csv("./data/raw/diabetes.csv", header = TRUE)
saveRDS(datacsv, "./data/raw/dataset.rds")

# Preprocessing
dataRDS <- readRDS("./data/raw/dataset.rds")
dataRDS <- na.omit(dataRDS) # removes NA from important columns
saveRDS(dataRDS, "./data/processed/dataset.rds")
dataset <- readRDS("./data/processed/dataset.rds")

# Recoding BMI to classes
dataRDS$BMI[dataRDS$BMI <= 18.5 ] <- 1
dataRDS$BMI[dataRDS$BMI > 18.5 & dataRDS$BMI <= 25  ] <- 2
dataRDS$BMI[dataRDS$BMI > 25 & dataRDS$BMI <= 30 ] <- 3
dataRDS$BMI[dataRDS$BMI > 30] <- 4

# Removing missing values
dataRDS <- subset(dataRDS,
                     Glucose != 0 &
                     Insulin != 0 &
                     BMI != 0 &
                     SkinThickness != 0)

```

## **Exploring the data**

In the following section we will explore each part of the dataset to get a full understanding of our dataset. We do this using a range of histograms that show the distribution of each value with an interpretation of why that data matters.

```{r}
#| warning: false
#| echo: false
for (i in 1:1) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "pink",
       border = "white")
}
```

-   **Pregnancies:** number of times pregnant, represented as an integer, values range between 0-14

Pregnancies is a fairly uninteresting distribution, only a couple people get pregnant more than a few times. Which makes sense! What is good to know is that pregnancies is highly influenced by age, because young people usually have not been pregnant often

```{r}
#| echo: false
for (i in 2:2) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "green",
       border = "white")
}
```

-   **Glucose:** plasma glucose concentration in a toleration test represented as an integer, values range from 56-198.

There is not a lot to say about the glucose distribution, it maintains the above stated values with a seemingly normal distribution that fits within norms of a healthy person.

```{r}
#| echo: false
for (i in 3:3) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "red",
       border = "white")
}
```

-   **BloodPressure:** Bloodpressure represented as an integer with values in the range of 24-110.

Bloodpressure is more interesting! It ranges wide enough to question the validity of some datapoints, namely some values of bloodpressure are too low to sustain a healthy person. Meaning some of the data we use can be flawed or perhaps taken from patients in intensive care situations.

```{r}
#| echo: false
for (i in 4:4) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "beige",
       border = "white")
}
```

-   **Skinthickness:** Thickness of the skin in millimetres, represented as an integer with values ranging from 7-63.

Skinthickness represents the thickness of the skin under the upper arm, which ranges in a normal distribution, no inferences are made using this graph so not a lot can be said about its value.

```{r}
#| echo: false
for (i in 5:5) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
```

-   **Insulin:** ‚Äú2 hour serum insulin‚Äù with values ranging from 14-846.

Insulin is a very important variable measured in units per milliliter with a wide range including absurdly high values. Values ranging from 20-200 can be reasonably explained but the same cannot be said about values 800+. We assume this data could be invalid or taken right after an insulin injection which is why these datapoints are not yet removed.

```{r}
#| echo: false
for (i in 6:6) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "orange",
       border = "white")
}
```

-   **BMI:** BMI value represented as classification, namely: underweight=1, healthy=2, overweight=3 and obese=4. Most people in this study are considered to be obese.

The most interesting part of this data is that 259/393 = 66% are obese which means our model will most likely not be good at predicting glucose for people who are underweight and less good for healthy and overweight people. While this model will be the best for predicting glucose in severely overweight people. Keep this in mind when we interpret our model.

```{r}
#| echo: false
#| warning: false
for (i in 7:7) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}

```

-   **DiabetesPedigreeFunction:** Quantified genetic risk of diabetes. Risks range from 0-2,24.

The diabetes pedigree function is a function that predicts diabetes using hereditary information, values in this dataset conform to the limits of the function and range normally on lower values with some extremer values on the high end of the values. Due to this fitting within function norms there is no reason to assume this data is invalid.

```{r}
#| echo: false
for (i in 8:8) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "black",
       border = "white")
}

```

-   **Age:** Integer representing age, values range from 21-81.

Age ranges widely in our dataset, it largely ranges between 21 and 60 with only a couple values above 60. Keep this in mind while interpreting our model for this means the model is not properly trained on anyone under 21 and over 60. Meaning these are the ages we likely want to use our model on.

```{r}
#| echo: false
#| warning: false
for (i in 9:9) {
  col_name <- colnames(dataRDS)[i]
  hist(dataRDS[, i],
       main = paste("Histogram of ", col_name),
       xlab = paste("Values of", col_name),
       col = "lightblue",
       border = "white")
}
```

-   **Outcome:** 0 or 1 value representing if someone has diabetes or not.

Most participants don‚Äôt have diabetes in our model but values are fairly balanced by about a 2:1 split, however because we are trying to predict glucose to help people find diabetes earlier in their lifetime we will not be using this value in our model.

```{r}
#| echo: false

library(GGally)
ggpairs(dataRDS) 

```

In this pairs plot you can see how each value interacts with each other. Most predictors do not strongly correlate with each other with the exception of a few, namely pregnancies and age, glucose and insulin or glucose and outcome. These values are logically very closely related, for example the older you are the more pregnancies you can have. Insulin helps process glucose which directly influences it's values. A higher percentage of glucose is expected when you suffer from diabetes.

## **Building the model**

```{r}
#| warning: false
#| echo: false

# --- Model Definitions ---

models <- list(
  
  # 1. Exploratory models
  mod2 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness + DiabetesPedigreeFunction,
            data = dataRDS),
            
  mod3 = lm(Glucose ~ Age + Insulin + BMI + SkinThickness, 
            data = dataRDS),
            
  mod4 = lm(Glucose ~ Insulin + BMI + SkinThickness + Age * Pregnancies, 
            data = dataRDS),
            
  mod6 = lm(Glucose ~ Age + Insulin + BMI, 
            data = dataRDS),

  # 2. Systematic approach
  
  m1_full = lm(Glucose ~ Age + Insulin + BMI + Pregnancies + BloodPressure + 
               SkinThickness + DiabetesPedigreeFunction, 
               data = dataRDS),
  
  m2_reduced = lm(Glucose ~ Age + Insulin + BMI + SkinThickness + DiabetesPedigreeFunction, 
                  data = dataRDS),
  
  m3_core = lm(Glucose ~ Age + Insulin + BMI + SkinThickness, data = dataRDS),
  
  m4_metabolic = lm(Glucose ~ Insulin + BMI + SkinThickness,
                    data = dataRDS),
  
  m5_simple = lm(Glucose ~ Insulin + BMI,
                 data = dataRDS),
  
  m6_minimal = lm(Glucose ~ BMI,
                  data = dataRDS),

  # 3. Models with moderation
  
  m7_age_preg = lm(Glucose ~ Age * Pregnancies + Insulin + BMI + SkinThickness,
                   data = dataRDS),
  
  m8_insulin_bmi = lm(Glucose ~ Insulin * BMI + Age + SkinThickness,
                      data = dataRDS),
  
  m9_insulin_skin = lm(Glucose ~ Insulin * SkinThickness + Age + BMI,
                       data = dataRDS),
  
  m10_bmi_skin = lm(Glucose ~ BMI * SkinThickness + Age + Insulin,
                    data = dataRDS),
  
  m11_genetic_bmi = lm(Glucose ~ DiabetesPedigreeFunction * BMI + Age + Insulin + SkinThickness,
                       data = dataRDS),

  # 4. Final candidates
  
  model4  = lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
               data = dataRDS), 
               
  model35 = lm(Glucose ~ Insulin * Age + BloodPressure, 
               data = dataRDS), # Lowest AIC/BIC
               
  model36 = lm(Glucose ~ Insulin * Age + BloodPressure + BMI, 
               data = dataRDS) 
)

# Model Comparison used to compare many models 
comparison <- data.frame(
  Model  = names(models),
  AIC    = sapply(models, AIC),
  BIC    = sapply(models, BIC),
  Adj_R2 = sapply(models, function(m) summary(m)$adj.r.squared)
)
#comparison[order(comparison$AIC), ]

discussed_models  <- list(
  model4  = lm(Glucose ~ Age + Insulin + BMI + BloodPressure,
               data = dataRDS), 
               
  model35 = lm(Glucose ~ Insulin * Age + BloodPressure, 
               data = dataRDS), # Lowest AIC/BIC
               
  model36 = lm(Glucose ~ Insulin * Age + BloodPressure + BMI, 
               data = dataRDS)  
)
discussed_models_comparison <- data.frame(
  #Model = names(discussed_models),
  AIC = sapply(discussed_models, AIC),
  BIC = sapply(discussed_models, BIC),
  Adj_R2 = sapply(discussed_models, function(x) summary(x)$adj.r.squared)
)
discussed_models_comparison
# Final model

model4 <- lm(Glucose ~ Age + Insulin + BMI + BloodPressure, data = dataRDS)

```

## **Model discussion**

The model development began with an exploratory phase where various combinations of variables were tested based on individual significance and theoretical relevance. We then followed a systematic "Full-to-Minimal" approach, starting with a model that included all predictors: pregnancies, age, insulin, BMI, blood pressure, skin thickness, and the diabetes pedigree function. By removing variables with the least amount of explanatory power, we created simplified models focused on metabolic indicators. In addition to additive models, we made several moderations to test whether variables influenced each other, such as the moderation between age and pregnancies or insulin and BMI.

Through keep comparing the models by statistical fit and predictor correlations, we narrowed our analysis to three final models: model4, model35, and model36. These candidates were identified as the most robust configurations after evaluating their AIC and BIC scores to balance explanatory power with model simplicity.

To find the most suitable model for predicting glucose levels, we compared various models based on the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The table above shows that model35, including the moderation Age\*Insulin, has the lowest AIC and BIC values. From a statistical perspective and purely based on the AIC and BIC scores, this appears to be the model that gives the most information with the least amount of complexity.

However, better inspection of model35 and model36 showed that the relationship between the predictors and the dependent variable glucose were not fully linear. This would violate a a fundamental assumption of linear regression. Next to that, both models had predictors with high VIF scores as can be seen in the table below. This would mean that the estimates of the regression coefficients would become unreliable and that the model is unable to distinguish which effect belongs to which variable. Because of the fact that both models were violating the linearity assumption and that both models had predictors with high VIF score, we decided that we had to go for another model.

The VIF scores for the predictors of model35 and model36 respectively:

```{r}
#| echo: false
model35 <- lm(Glucose ~ Insulin * Age + BloodPressure, 
               data = dataRDS) # Lowest AIC/BIC
               
model36 <- lm(Glucose ~ Insulin * Age + BloodPressure + BMI, 
               data = dataRDS)  

VIF(model35)
VIF(model36)
```

Therefore, we have chosen model 4:

model4 = lm(Glucose \~ Age + Insulin + BMI + BloodPressure, data = dataRDS)

Although the AIC and BIC are slightly higher than in model35 and model36, model4 still performs significantly strong. It is an additive model that combines medically logical predictors such as age, insulin, BMI, and blood pressure without any interactions. This not only makes the model more interpretable, but also ensures we remain within the constraints of linear regression.

## **Assumptions**

We will now discuss and check the various model assumptions of linear regression.

### Linearity

With the assumption of linearity, it is assumed that the relation between the dependent and independent variables is more or less linear. If this assumption would be violated, we could not be doing a linear regression and we should move to an entirely different modeling paradigm.

Because it is multiple regression, a basic residual plot is not sufficient as it would not tell us which predictors exhibit nonlinear associations. Instead, Component + Residual Plots or partial residual plots can be used to visualize the unique effects of each predictor.

```{r}
#| echo: false

crPlots(model4)
```

In the Component + Residual Plots it is clear that all predictors seem to have a (near) linear relationship with the Component+Residual. For Insulin there seems to be a kink in the otherwise linear relationship. We consider this as sufficient to not violate the linearity assumption.

### Predictor matrix is full rank

This assumption states that there need to be more observations than predictors and no predictor can be a linear combination of other predictors, meaning predictors cannot have a very high correlation (multicollinearity). If this assumption were to be violated, the parameter estimates could not be uniquely determined from the data.

To check for this assumption, we use the Variance Inflation Factor (VIF) to determine multicollinearity.

```{r}
#| echo: false

VIF(model4)
```

A VIF-score should be interpreted as follows:

-   VIF=1: this means no multicollinearity;

-   VIF=5: this implies already quite some multicollinearity;

-   VIF=10: this implies problematic amounts of multicollinearity.

Because the VIF-scores for all four predictors is near 1, this assumption is not violated.

### Exogenous predictors

For this assumption, the expected value of the errors must be 0. Furthermore, the errors must be independent of the predictors. This entails that:

-   $Cov(\hat Y,\epsilon)=0$

-   $E[\epsilon_n]=0$

Because this is more a model assumption issue rather than something to directly test as the error term is unobserved. We assume the predictors not to be endogenous. Not meeting this assumption results in biased estimates for the regression coefficients.

### Constant, finite error variance

This assumption is the assumption of homoscedasticity. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values.

If this assumption would be violated, the estimated standard errors would be biased (usually downward). This would entail that test statistics would be too large, CIs wouldbe too narrow and we would have inflated Type I error rates. The heteroscedasticity will not bias the parameter estimates, meaning the best fit line would still be correct but our measure of uncertainty around that best fit line would be wrong.

```{r}
#| echo: false

model4 %>%
  plot(1)
```

In the plot it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. This indicates the this assumption is not violated.

### Independent errors

This assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.

Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).

Our model does not seem to contain predictors that are measured over time, therefore we assume this to not be the case.

We check for clustering using the ICC scores for pregnancies, skin thickness and the diabetes pedigree function respectively.

```{r}
#| warning: false
#| echo: false

ICC::ICCbare(x=dataRDS$Pregnancies, y=resid(model4))
ICC::ICCbare(x=dataRDS$SkinThickness, y=resid(model4))
ICC::ICCbare(x=dataRDS$DiabetesPedigreeFunction, y=resid(model4))
```

For Pregnancies and Skin thickness, we see negligible clustering with an ICC between 0 and 0.05. For the diabetes pedigree function we see an ICC between 0.05 and 0.10, which indicates small clustering. With this, we conclude that this assumption is not violated.

### Normally distributed errors

This assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked using model plots.

```{r}
#| echo: false

model4 %>%
  plot(2)
```

The Q-Q plot shows that the error terms follow the ideal line pretty well, thus the assumption holds.

If the assumption would have been violated, it would imply a misspecified model. Then the justification for some tests and procedures used in regression analysis might not hold. For prediction intervals, normality of errors is always desired.

------------------------------------------------------------------------

## Influential Observations

### Outliers

Outliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the externally studentized residuals.

```{r}
#| echo: false

model4 %>%
  rstudent() %>%
  plot()
```

There does not appear to be a clear outlier in this plot.

### High-leverage observations

High-leverage observations are observations with extreme predictor values. To detect these observations, we look at their leverage values. These values are be summarized in a leverage plot.

```{r}
#| echo: false

model4 %>%
  hatvalues() %>%
  plot()
```

Here we see a few possible outstanding observations, like the observation close to the beginning, two observations around 100 and an observation somewhere between observation 200 and 300. Looking in the dataset, these outstanding observations can be explained by their high values for Insulin. For example, index 5 has an insulin of 846, which is the largest of the dataset. The other points standing out also have an outstanding insulin value.

A case with high leverage is not necessarily bad, the influence on the model is more important.

### Influence on the model

Both outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem.

Influence measures come in two sorts: Cook's distance checks for influential observations, while DFBETAS check for influential, and possibly problematic, observations per regression coefficient.

First we check the Cook's distance.

```{r}
#| echo: false

model4 %>%
  cooks.distance() %>%
  plot()
```

Here we see outstanding distances around the beginning and around 300. Next we check the DFBETAS.

```{r}
#| echo: false

plot(dfbetas(model4)[,1],main="intercept")
plot(dfbetas(model4)[,2],main="slope")
```

In the DFBETAS plots, we see that different points are most outstanding, also different points as in the Cook's distance plot. From this we conclude that there is no reason to assume any of these points are problematic.

## **Exploring the model**

```{r}
#| warning: false
#| echo: false
summary(model4)
```

A multiple linear regression was conducted to examine the predictors for glucose levels. This was done through the use of age, insulin, BMI, and blood pressure. The overall model was significant (F(4, 388) = 64.49, p \> .01) and explains 39.94% of the variance in glucose (R¬≤ = .3994). Age has a positive effect on glucose levels where older individuals have a higher glucose level (ùõΩ = .59, p \< .01). The same is true for insulin levels (ùõΩ = .13, p \< .01) where insulin increases when glucose levels go up. Although not as significant as age and insulin, blood pressure shows a significant effect on the increase of glucose levels (ùõΩ = .21, p \< .05). BMI (ùõΩ = 2.72, p = .13) shows a positive effect, but not a significant one. We still kept BMI in the model to explain glucose levels, since, from a logical standpoint, glucose levels are linked to BMI. Overall, the results suggest that only age, insulin, and blood pressure contribute significantly to glucose level variation.

## **Conclusion**

To answer our research question, the results indicate that glucose levels can be predicted by age, insulin, and blood pressure which all showed a positive relation with glucose: as age, insulin, and blood pressure increase, glucose levels will also increase. BMI did not significantly predict glucose. This highlights that, within this sample, glucose levels are in part accounted for by age, insulin levels, and blood pressure rather than body composition.

## Literature

Cijfers over diabetes. (n.d.). diabetes.nl. https://www.diabetes.nl/wat-is-diabetes/over-diabetes/cijfers- over-diabetes

Diabetes Mellitus type 2 \| LUMC. (n.d.). https://www.lumc.nl/afdelingen/endocrinologie/ziektes-en-aandoeningen/diabetes-mellitus-type-2/

Umcu. (n.d.). Diabetes. UMC Utrecht. https://www.umcutrecht.nl/nl/ziekte/diabetes

Akturk, M. & National Institute of Diabetes and Digestive and Kidney Diseases. (2020). Diabetes \[Dataset\]. https://www.kaggle.com/datasets/mathchi/diabetes-data-set
